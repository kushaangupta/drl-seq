{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "4zW6CU8F69Zp"
   },
   "source": [
    "## Reinforcement learning example with stable-baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzF5leN1R2xU"
   },
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1vb3YFfIoOA4"
   },
   "source": [
    "here we build the Random Dots Motion task, specifying the duration of each trial period (fixation, stimulus, decision) and wrapp it with the pass-reward wrapper which appends the previous reward to the observation. We then plot the structure of the task in a figure that shows: \n",
    "1. The observations received by the agent (top panel). \n",
    "2. The actions taken by a random agent and the correct action at each timestep (second panel).\n",
    "3. The rewards provided by the environment at each timestep (third panel).\n",
    "4. The performance of the agent at each trial (bottom panel).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "RaH9CcJdHY5G",
    "outputId": "25cc6eaa-0531-4e53-df94-c8fe3979aaf0"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import neurogym as ngym\n",
    "from neurogym.wrappers import pass_reward\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Task name\n",
    "name = 'contrib.SequenceAlternation-v0'\n",
    "# task specification (here we only specify the duration of the different trial periods)\n",
    "rewards =  {'correct': +1., 'fail': 0.}\n",
    "\n",
    "kwargs = {'dt': 100}# 'rewards': rewards, 'opponent_type': opponent_type, 'learning_rate': learning_rate}\n",
    "\n",
    "# build task\n",
    "env = gym.make(name, **kwargs)\n",
    "# print task properties\n",
    "print(env)\n",
    "\n",
    "# wrapp task with pass-reward wrapper\n",
    "env = pass_reward.PassReward(env)\n",
    "# plot example trials with random agent\n",
    "data = ngym.utils.plot_env(\n",
    "    env, fig_kwargs={'figsize': (12, 12)}, num_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OCFMPbzX38Wj"
   },
   "source": [
    "### Train a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jAxTPbzL38Wl",
    "outputId": "b7370af5-5628-4cb3-e734-3accfe0fb0e9"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import A2C  # ACER, PPO2\n",
    "\n",
    "# # Optional: PPO2 requires a vectorized environment to run\n",
    "# # the env is now wrapped automatically when passing it to the constructor\n",
    "# env = DummyVecEnv([lambda: env])\n",
    "\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000, log_interval=1000)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svUQlptJAVv9"
   },
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qPrh-kiI8rbB",
    "outputId": "be1e5cbf-23e0-49ca-8468-a93836888772"
   },
   "outputs": [],
   "source": [
    "env = gym.make(name, **kwargs)\n",
    "# print task properties\n",
    "print(env)\n",
    "# wrapp task with pass-reward wrapper\n",
    "env = pass_reward.PassReward(env)\n",
    "# env = DummyVecEnv([lambda: env])\n",
    "# plot example trials with random agent\n",
    "data = ngym.utils.plot_env(env, fig_kwargs={'figsize': (12, 12)}, num_steps=100, model=model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "example_neurogym_rl.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
